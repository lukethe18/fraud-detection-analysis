# -*- coding: utf-8 -*-
"""Synthetic Fraud Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sZMdDHrXeK6fScDVh2ZN-tVdp7tLVa52

# Credit Card Fraud Feature Engineering, Regression, and Deep Learning

Code and analysis by Luke Theivagt


The data used for this project was collected from a fake transaction data generator.
https://github.com/namebrandon/Sparkov_Data_Generation/tree/master
"""

!pip install category_encoders
!pip install imblearn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

from imblearn.over_sampling import SMOTENC

import category_encoders as ce

from collections import Counter

from xgboost import XGBClassifier

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Data Analysis Projects/fraudTrain.csv')

CCData = data

"""# EDA"""

CCData.tail()

CCData.shape

CCData.info()

CCData['is_fraud'].value_counts()
Percent_Fraud = round((CCData['is_fraud'].value_counts()[1]/CCData.shape[0] *100),2)
print(Percent_Fraud, "% of transactions are fraudulent")

"""Oversampling and Undersampling techniques will be nessicairy as the fradulent cases take up such a small part of the total dataset."""

CCData['category'].value_counts()

fraud_data = CCData[CCData['is_fraud'] == 1]

fig, ax = plt.subplots(figsize=(8, 6))
sns.histplot(fraud_data['amt'], bins=50, kde=True, ax=ax, color='red')
ax.set_title('Distribution of Transaction Amounts (Fraudulent)')
ax.set_xlabel('Transaction Amount')
ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

non_fraud_data = CCData[CCData['is_fraud'] == 0]

fig, ax = plt.subplots(figsize=(8, 6))
# Apply a logarithmic transformation to 'amt' to better visualize the distribution
sns.histplot(np.log1p(non_fraud_data['amt']), bins=50, kde=True, ax=ax, color='blue')
ax.set_title('Distribution of Log-Transformed Transaction Amounts (Non-Fraudulent)')
ax.set_xlabel('Log(1 + Transaction Amount)')
ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""There was a large frequency of 1$ transactions, making it impossible to visualize the full distribution of transactions. Therefore I applied a logarithmic transformation to more effectively visualize the distribution."""

category_fraud_counts = CCData.groupby(['category', 'is_fraud']).size().unstack(fill_value=0)

category_fraud_counts.plot(kind='bar', stacked=True, figsize=(12, 6))
plt.title('Fraudulent vs. Non-Fraudulent Transactions by Category')
plt.xlabel('Category')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Is Fraud', labels=['No Fraud', 'Fraud'])
plt.tight_layout()
plt.show()

state_fraud_counts = data.groupby(['state', 'is_fraud']).size().unstack(fill_value=0)
state_fraud_counts['Total'] = state_fraud_counts[0] + state_fraud_counts[1]

# Avoid division by zero if a state has no transactions
state_fraud_counts['Fraud_Percentage'] = (state_fraud_counts[1] / state_fraud_counts['Total']) * 100
top_10_fraud_states = state_fraud_counts.sort_values(by='Fraud_Percentage', ascending=False).head(10)

plt.figure(figsize=(12, 7))
sns.barplot(x=top_10_fraud_states.index, y=top_10_fraud_states['Fraud_Percentage'],
            hue = top_10_fraud_states.index, palette='viridis', legend = False)
plt.title('Top 10 States by Fraud Percentage')
plt.xlabel('State')
plt.ylabel('Fraud Percentage (%)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

display(top_10_fraud_states)

category_fraud_counts = CCData.groupby(['category', 'is_fraud']).size().unstack(fill_value=0)
category_fraud_counts['Total'] = category_fraud_counts[0] + category_fraud_counts[1]
category_fraud_counts['Fraud_Percentage'] = (category_fraud_counts[1] / category_fraud_counts['Total']) * 100

top_5_fraud_categories = category_fraud_counts.sort_values(by='Fraud_Percentage', ascending=False).head(5)
display(top_5_fraud_categories)

"""The shopping_net category sees the highest percentage of fraud."""

city_fraud_counts = data.groupby(['city', 'is_fraud']).size().unstack(fill_value=0)
city_fraud_counts['Total'] = city_fraud_counts[0] + city_fraud_counts[1]

city_fraud_counts['Fraud_Percentage'] = (city_fraud_counts[1] / city_fraud_counts['Total']) * 100
top_10_fraud_cities = city_fraud_counts.sort_values(by='Fraud_Percentage', ascending=False).head(10)

display(top_10_fraud_cities)

"""There are many cities that see 100% fraud, meaning it will likely be a useful metric for detetermining and fradulent transaction."""

zip_fraud_counts = data.groupby(['zip', 'is_fraud']).size().unstack(fill_value=0)
zip_fraud_counts['Total'] = zip_fraud_counts[0] + zip_fraud_counts[1]

zip_fraud_counts['Fraud_Percentage'] = (zip_fraud_counts[1] / zip_fraud_counts['Total']) * 100
top_10_fraud_zip = zip_fraud_counts.sort_values(by='Fraud_Percentage', ascending=False).head(10)

display(top_10_fraud_zip)

CCData['category'] = CCData['category'].astype('category')
CCData['job'] = CCData['job'].astype('category')
CCData['zip'] = CCData['zip'].astype('category')
CCData['city'] = CCData['city'].astype('category')
CCData['state'] = CCData['state'].astype('category')

"""# Feature Engineering: Creating Features and Encoding Categories

Date time is converted to specific categories and various categories are converted preserve magnitute of the relationship, increase dimensionality, and remove any simulated ordinal relationship. Zip code, job, and category will be binary encoded to save memory usage.

Waking hours are set from 6:00 a.m. to midnight. People have many various schedules, but I feel like most people do not often make purchaces outside of this time.
"""

# Converting date time into more specific categories

CCData['trans_date_trans_time'] = pd.to_datetime(CCData['trans_date_trans_time']) # Convert to datetime first

CCData['trans_hour'] = CCData['trans_date_trans_time'].dt.hour
CCData['trans_dayofweek'] = CCData['trans_date_trans_time'].dt.dayofweek
CCData['trans_month'] = CCData['trans_date_trans_time'].dt.month


# 0 if 12:00 to 6:00; 1 otherwise
CCData['trans_waking_hour'] = CCData['trans_hour'].apply(lambda x: 0 if 0 <= x <= 6 else 1)

if 'trans_dayofweek' in CCData.columns:
    day_of_week_encoded = pd.get_dummies(CCData['trans_dayofweek'], prefix='dayofweek', drop_first=True)
    CCData = pd.concat([CCData, day_of_week_encoded], axis=1)
    CCData = CCData.drop('trans_dayofweek', axis=1)

if 'gender' in CCData.columns:
    gender_encoded = pd.get_dummies(CCData['gender'], prefix='gender', drop_first=True)
    CCData = pd.concat([CCData, gender_encoded], axis=1)

if 'week_day' in CCData.columns:
    CCData = CCData.drop('week_day', axis=1)

"""Binary encoding zip, category, job, city, and state."""

CCData['Zip_Code'] = LabelEncoder().fit_transform(CCData['zip'])
CCData['Zip_Code'] = CCData['Zip_Code'].apply(lambda x: format(x, 'b'))
CCData['Zip_Code'] = CCData['Zip_Code'].apply(lambda x: [int(i) for i in x])

max_len = CCData['Zip_Code'].apply(len).max()
zipcode_columns = [f'Zip_Code_{i}' for i in range(max_len)]
CCData[zipcode_columns] = pd.DataFrame(CCData['Zip_Code'].tolist(), index=CCData.index)

#binary encoding merchant category
CCData['Merchant_category'] = LabelEncoder().fit_transform(CCData['category'])
CCData['Merchant_category'] = CCData['Merchant_category'].apply(lambda x: format(x, 'b'))
CCData['Merchant_category'] = CCData['Merchant_category'].apply(lambda x: [int(i) for i in x])

max_len = CCData['Merchant_category'].apply(len).max()
category_columns = [f'Merchant_category{i}' for i in range(max_len)]
CCData[category_columns] = pd.DataFrame(CCData['Merchant_category'].tolist(), index=CCData.index)

#Binary encoding cardholder job
CCData['Cardholder_Job'] = LabelEncoder().fit_transform(CCData['job'])
CCData['Cardholder_Job'] = CCData['Cardholder_Job'].apply(lambda x: format(x, 'b'))
CCData['Cardholder_Job'] = CCData['Cardholder_Job'].apply(lambda x: [int(i) for i in x])

#Binary encoding city
CCData['City'] = LabelEncoder().fit_transform(CCData['city'])
CCData['City'] = CCData['City'].apply(lambda x: format(x, 'b'))
CCData['City'] = CCData['City'].apply(lambda x: [int(i) for i in x])

max_len = CCData['City'].apply(len).max()
zipcode_columns = [f'City_{i}' for i in range(max_len)]
CCData[zipcode_columns] = pd.DataFrame(CCData['City'].tolist(), index=CCData.index)

#Binary encoding month
CCData['trans_month'] = LabelEncoder().fit_transform(CCData['trans_month'])
CCData['trans_month'] = CCData['trans_month'].apply(lambda x: format(x, 'b'))
CCData['trans_month'] = CCData['trans_month'].apply(lambda x: [int(i) for i in x])

"""Creating Feature for customer age."""

#Use DOB to create a category for cardholder age
CCData['DOB'] = pd.to_datetime(CCData['dob'])
CCData['Cardholder_Age'] = CCData['trans_date_trans_time'].dt.year - CCData['DOB'].dt.year
CCData = CCData.drop('DOB', axis=1)

"""Feature for distance between transaction and billing address."""

def haversine_distance(row):
    """
    Calculates the Haversine distance (in kilometers) between two sets of
    latitude and longitude points.
    """
    R = 6371
    lat1_rad = np.radians(row['lat'])
    lon1_rad = np.radians(row['long'])
    lat2_rad = np.radians(row['merch_lat'])
    lon2_rad = np.radians(row['merch_long'])

    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad

    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

    distance = R * c
    return distance

CCData['distance_km'] = CCData.apply(haversine_distance, axis=1)

CCData.info()

"""Feature for the z score of purchase ammount per credit card."""

df = CCData.sort_values(['cc_num', 'trans_date_trans_time']).reset_index(drop=True)

df['user_avg_amount'] = df.groupby('cc_num')['amt'].transform('mean')
df['user_std_amount'] = df.groupby('cc_num')['amt'].transform('std')

df['user_std_amount'] = df['user_std_amount'].fillna(0)

CCData['amount_zscore'] = (df['amt'] - df['user_avg_amount']) / df['user_std_amount'].replace(0, 1)

"""# Regression Model

"""

y = CCData["is_fraud"]
columns_to_keep = [
    'amt', 'distance_km', 'city_pop', 'trans_hour',
    'trans_waking_hour', 'Cardholder_Age', 'dayofweek_1', 'dayofweek_2', 'dayofweek_3', 'dayofweek_4', 'dayofweek_5', 'dayofweek_6',
    'gender_M', 'amount_zscore', 'Zip_Code_2','Zip_Code_3','Zip_Code_5','Zip_Code_6',
    'Zip_Code_7','Zip_Code_8','Zip_Code_9', 'Merchant_category0','Merchant_category1','Merchant_category2',
    'Merchant_category3', 'City_0', 'City_1','City_2','City_3','City_4','City_5','City_6','City_7','City_8','City_9']
X = CCData[columns_to_keep]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

X_train = X_train.fillna(0)
X_test = X_test.fillna(0)

scaler = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

y_train_aligned = y_train.reset_index(drop=True)
y_test_aligned = y_test.reset_index(drop=True)

X_train_final = sm.add_constant(X_train_scaled, prepend=False)
X_test_final = sm.add_constant(X_test_scaled, prepend=False)

logit_mod = sm.Logit(y_train_aligned, X_train_final)
logit_res = logit_mod.fit_regularized(alpha=1e-4, L1_wt=0) # L1_wt=0 so L2 (ridge) regularization

print(logit_res.summary())

y_pred_prob = logit_res.predict(X_test_final)

"""The model shows that higher transaction amounts and certain transaction times are strong predictors of fraud. Transactions during waking hours are much less likely to be fraudulent, while specific hours and mid-week days slightly increase risk. A few location variables and cardholder age have small but significant effects.

# Sampling Techniques
"""

dfcolumns_to_keep = [
    'amt', 'distance_km', 'city_pop', 'trans_hour', 'gender', 'category',
    'trans_waking_hour', 'Cardholder_Age', 'amount_zscore','zip', 'merchant','state', 'city']
X = CCData[dfcolumns_to_keep]
y = df['is_fraud']

X.info()

"""Spliting features and seperating into training and testing."""

HIGH_CARDINALITY_COLS = ['zip', 'merchant','state', 'city']
OTHER_CATEGORICAL_COLS = ['category', 'gender', 'trans_waking_hour']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

"""Target encoding for high-cardinality features"""

target_encoder = ce.TargetEncoder(
    cols=HIGH_CARDINALITY_COLS,
    smoothing=0.2)

target_encoder.fit(X_train, y_train)

X_train_encoded = target_encoder.transform(X_train)
X_test_encoded = target_encoder.transform(X_test)

print(f"Target Encoding Complete. {HIGH_CARDINALITY_COLS} is now a numerical feature.")

"""Encoding remaining catigorical colunms."""

for col in OTHER_CATEGORICAL_COLS:
    X_train_encoded[col] = X_train_encoded[col].astype('category')
    X_test_encoded[col] = X_test_encoded[col].astype('category')

categorical_features_indices = [
    X_train_encoded.columns.get_loc(col)
    for col in OTHER_CATEGORICAL_COLS
]

"""SMOTE initializing and resampling"""

print("-" * 30)
print(f"Original training set distribution: {Counter(y_train)}")

# Initialize SMOTE-NC with  indices of remaining categorical columns
smote_nc = SMOTENC(
    categorical_features=categorical_features_indices,
    random_state=42
)

X_resampled, y_resampled = smote_nc.fit_resample(X_train_encoded, y_train)

print("-" * 30)
print(f"Resampled training set distribution: {Counter(y_resampled)}")
print("\nResampling complete. The fraud class (1) has been balanced.")

print("\n" + "="*40)
print("FINAL ENCODING BEFORE MODEL TRAINING")
print("="*40)

X_resampled_final = pd.get_dummies(X_resampled,
                                     columns=OTHER_CATEGORICAL_COLS,
                                     drop_first=False)

X_test_final = pd.get_dummies(X_test_encoded,
                                columns=OTHER_CATEGORICAL_COLS,
                                drop_first=False)

train_cols = X_resampled_final.columns
X_test_aligned = X_test_final.reindex(columns=train_cols, fill_value=0)

print("Final Data Check:")
print(f"X_resampled_final shape: {X_resampled_final.shape}")
print(f"X_test_aligned shape: {X_test_aligned.shape}")
print(f"Columns match: {all(X_resampled_final.columns == X_test_aligned.columns)}")

"""# Random Forest

Initializing and fiting the model
"""

print("Training Random Forest Classifier...")

rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced',
    n_jobs=-1
)

rf_model.fit(X_resampled_final, y_resampled)

print("Training complete")

"""Predicition and evaluation"""

y_pred = rf_model.predict(X_test_aligned)

y_pred_proba = rf_model.predict_proba(X_test_aligned)[:, 1]

print("\n" + "="*50)
print("             MODEL EVALUATION ON  TEST DATA")
print("="*50)

# Classification Report (Precision, Recall, F1-Score)
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# Area Under the ROC Curve (ROC AUC)
# ROC AUC should be used for imbalanced data
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\n--- ROC AUC Score ---")
print(f"ROC AUC: {roc_auc:.4f}")

cm = confusion_matrix(y_test, y_pred)
print("\n--- Confusion Matrix ---")
print("       Predicted 0 | Predicted 1")
print(f"Actual 0 |   {cm[0, 0]}    |    {cm[0, 1]}") # True Negatives, False Positives
print(f"Actual 1 |   {cm[1, 0]}    |    {cm[1, 1]}") # False Negatives, True Positives

# Interpretation of the Confusion Matrix:
TN = cm[0, 0] # Correctly identified non-fraud
FP = cm[0, 1] # Non-fraud misidentified as fraud
FN = cm[1, 0] # Fraud misidentified as non-fraud
TP = cm[1, 1] # Correctly identified fraud

print(f"\nKey Metrics for Fraud Class (1):")
print(f"True Positives (TP - Caught Fraud): {TP}")
print(f"True Negative (TP - Caught Non Fraud): {TN}")
print(f"False Negatives (FN - Non-Fraud Labeled Fraud): {FP}")
print(f"False Negatives (FN - Missed Fraud): {FN}")

"""# XG Boost

Initialize and fit the model
"""

print("Training XGBoost Classifier...")

xgb_model = XGBClassifier(
    objective='binary:logistic',
    n_estimators=100,
    learning_rate=0.1,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1,
    scale_pos_weight=1
)

xgb_model.fit(X_resampled_final, y_resampled)

print("Training complete.")

"""Model prediction and evaluation"""

y_pred = xgb_model.predict(X_test_aligned)

y_pred_proba = xgb_model.predict_proba(X_test_aligned)[:, 1]

print("\n" + "="*50)
print("XGBOOST EVALUATION ON TEST DATA")
print("="*50)

print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\n--- ROC AUC Score ---")
print(f"ROC AUC: {roc_auc:.4f}")

cm = confusion_matrix(y_test, y_pred)
print("\n--- Confusion Matrix ---")
print("       Predicted 0 | Predicted 1")
print(f"Actual 0 |   {cm[0, 0]}    |    {cm[0, 1]}")
print(f"Actual 1 |   {cm[1, 0]}    |    {cm[1, 1]}")